{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74617680",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "from datetime import date, timedelta\n",
    "from itertools import product\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sqlalchemy import create_engine, text\n",
    "from crepes import WrapRegressor\n",
    "from crepes.extras import DifficultyEstimator\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Folder of the current script (ML)\n",
    "script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# Absolute path to Logging_Script\n",
    "logging_script_dir = os.path.abspath(\n",
    "    os.path.join(script_dir, \"..\", \"Logging_Script\")\n",
    ")\n",
    "\n",
    "if logging_script_dir not in sys.path:\n",
    "    sys.path.append(logging_script_dir)\n",
    "\n",
    "from logging_config import setup_logger\n",
    "\n",
    "logger = setup_logger(\"PREDICTIONS\")\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"The number of unique classes is greater than 50% of the number of samples\",\n",
    ")\n",
    "\n",
    "today_date = date.today()\n",
    "# today_date = today_date - timedelta(days=1)\n",
    "#%%\n",
    "\n",
    "def preProcessingSteps(df_to_preprocess: pd.DataFrame):\n",
    "\n",
    "    logger.info(\"Preprocessing of Sales df started\")\n",
    "    try:\n",
    "        row_count = len(df_to_preprocess)\n",
    "        logger.info(\"Input dataframe rows: %d\", row_count)\n",
    "\n",
    "        min_date = df_to_preprocess['InvoiceDate'].min()\n",
    "        max_date = df_to_preprocess['InvoiceDate'].max()\n",
    "        logger.info(\"InvoiceDate range: %s -> %s\", min_date, max_date)\n",
    "\n",
    "        cutoff_date = max_date - timedelta(days=2*365)\n",
    "        if min_date < cutoff_date:\n",
    "            logger.info(\"Min date %s older than cutoff %s, adjusting\", min_date, cutoff_date)\n",
    "            min_date = cutoff_date\n",
    "        logger.info(\"Final date range used: %s -> %s\", min_date, max_date)\n",
    "\n",
    "        all_dates = [min_date + timedelta(days=i) for i in range((max_date - min_date).days + 1)]\n",
    "        Products = sorted(df_to_preprocess['Product'].dropna().unique())\n",
    "        Branch = sorted(df_to_preprocess['Branch'].dropna().unique())\n",
    "        logger.info(\"Unique Products: %d, Branches: %d\", len(Products), len(Branch))\n",
    "\n",
    "        grid_data = list(product(all_dates, Products, Branch))\n",
    "        full_grid = pd.DataFrame(grid_data, columns=['InvoiceDate', 'Product', 'Branch'])\n",
    "        logger.info(\"Full grid created with %d rows\", len(full_grid))\n",
    "\n",
    "        sales_agg = df_to_preprocess.groupby(['InvoiceDate', 'Product', 'Branch']).agg({'Quantity': 'sum'}).reset_index()\n",
    "        logger.info(\"Aggregated sales rows: %d\", len(sales_agg))\n",
    "\n",
    "        merged = full_grid.merge(sales_agg, on=['InvoiceDate', 'Product', 'Branch'], how='left').fillna(0)\n",
    "        logger.info(\"Merged dataset rows: %d\", len(merged))\n",
    "\n",
    "        Products = df_to_preprocess[['Product', 'TopCategory', 'LastCategory']].drop_duplicates()\n",
    "        Product_merge = merged.merge(Products, on='Product', how='left')\n",
    "\n",
    "        final_df = Product_merge.sort_values(['Product', 'Branch', 'InvoiceDate'])\n",
    "        final_df['Quantity'] = final_df['Quantity'].clip(lower=0)\n",
    "\n",
    "        final_df['month'] = final_df['InvoiceDate'].dt.month\n",
    "        final_df['day_of_month'] = final_df['InvoiceDate'].dt.day\n",
    "        final_df['day_of_week'] = final_df['InvoiceDate'].dt.dayofweek\n",
    "\n",
    "        final_df['is_weekend'] = (final_df['day_of_week'] >= 5).astype(int)\n",
    "        final_df['is_month_start'] = final_df['InvoiceDate'].dt.is_month_start.astype(int)\n",
    "        final_df['is_month_end'] = final_df['InvoiceDate'].dt.is_month_end.astype(int)\n",
    "\n",
    "        final_df = pd.get_dummies(final_df, columns=['month', 'day_of_month', 'day_of_week'],\n",
    "                                  prefix=['Month', 'Day', 'Weekday'], drop_first=False)\n",
    "        final_df = final_df.astype({col: 'int' for col in final_df.select_dtypes('bool').columns})\n",
    "        logger.info(\"Feature engineering completed\")\n",
    "\n",
    "        le_branch = LabelEncoder()\n",
    "        final_df['BranchEncoded'] = le_branch.fit_transform(final_df['Branch'])\n",
    "\n",
    "        tomorrow_date = final_df['InvoiceDate'].max() + timedelta(days=1)\n",
    "        logger.info(\"Tomorrow date generated: %s\", tomorrow_date)\n",
    "\n",
    "        tomorrow_combinations = final_df[['Product', 'Branch', 'TopCategory', 'LastCategory', 'BranchEncoded']].drop_duplicates()\n",
    "        tomorrow_combinations['InvoiceDate'] = tomorrow_date\n",
    "\n",
    "        tomorrow_combinations['month'] = tomorrow_combinations['InvoiceDate'].dt.month\n",
    "        tomorrow_combinations['day_of_month'] = tomorrow_combinations['InvoiceDate'].dt.day\n",
    "        tomorrow_combinations['day_of_week'] = tomorrow_combinations['InvoiceDate'].dt.dayofweek\n",
    "        tomorrow_combinations['is_weekend'] = (tomorrow_combinations['day_of_week'] >= 5).astype(int)\n",
    "        tomorrow_combinations['is_month_start'] = tomorrow_combinations['InvoiceDate'].dt.is_month_start.astype(int)\n",
    "        tomorrow_combinations['is_month_end'] = tomorrow_combinations['InvoiceDate'].dt.is_month_end.astype(int)\n",
    "\n",
    "        dummy_columns = [col for col in final_df.columns if col.startswith(('Month_', 'Day_', 'Weekday_'))]\n",
    "        tomorrow_encoded = pd.get_dummies(tomorrow_combinations, columns=['month', 'day_of_month', 'day_of_week'],\n",
    "                                          prefix=['Month', 'Day', 'Weekday'], drop_first=False)\n",
    "        tomorrow_encoded = tomorrow_encoded.astype({col: 'int' for col in tomorrow_encoded.select_dtypes('bool').columns})\n",
    "\n",
    "        missing_cols = 0\n",
    "        for col in dummy_columns:\n",
    "            if col not in tomorrow_encoded.columns:\n",
    "                tomorrow_encoded[col] = 0\n",
    "                missing_cols += 1\n",
    "        logger.info(\"Missing dummy columns added for tomorrow: %d\", missing_cols)\n",
    "\n",
    "        numeric_features = ['Quantity']\n",
    "        for col in numeric_features:\n",
    "            tomorrow_encoded[col] = np.nan\n",
    "\n",
    "        tomorrow_encoded = tomorrow_encoded[final_df.columns]\n",
    "        final_df = pd.concat([final_df, tomorrow_encoded], ignore_index=True)\n",
    "        logger.info(\"Preprocessing completed successfully, final rows: %d\", len(final_df))\n",
    "\n",
    "        return final_df, tomorrow_date\n",
    "\n",
    "    except Exception:\n",
    "        logger.critical(\"Preprocessing failed\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "def preForecastingSteps(df: pd.DataFrame, tomorrow_date):\n",
    "    logger.info(\"PreForecastingSteps started\")\n",
    "    df = df.sort_values(['Product', 'Branch', 'InvoiceDate'])\n",
    "\n",
    "    # Feature engineering: lag and rolling stats\n",
    "    for lag in range(1, 6):\n",
    "        df[f'lag_{lag}'] = df.groupby(['Product', 'Branch'])['Quantity'].transform(lambda x: x.shift(lag))\n",
    "    df['rolling_mean_3'] = df.groupby(['Product','Branch'])['Quantity'].transform(lambda x: x.shift(1).rolling(3).mean())\n",
    "    df['rolling_mean_7'] = df.groupby(['Product','Branch'])['Quantity'].transform(lambda x: x.shift(1).rolling(7).mean())\n",
    "    df['rolling_std_3']  = df.groupby(['Product','Branch'])['Quantity'].transform(lambda x: x.shift(1).rolling(3).std())\n",
    "\n",
    "    logger.info(\"Feature engineering completed: lag and rolling features added\")\n",
    "\n",
    "    # Base + lag + rolling features\n",
    "    base_features = [\n",
    "       'Month_1', 'Month_2', 'Month_3', 'Month_4', 'Month_5',\n",
    "       'Month_6', 'Month_7', 'Month_8', 'Month_9', 'Month_10', 'Month_11',\n",
    "       'Month_12', 'Day_1', 'Day_2', 'Day_3', 'Day_4', 'Day_5', 'Day_6',\n",
    "       'Day_7', 'Day_8', 'Day_9', 'Day_10', 'Day_11', 'Day_12', 'Day_13',\n",
    "       'Day_14', 'Day_15', 'Day_16', 'Day_17', 'Day_18', 'Day_19', 'Day_20',\n",
    "       'Day_21', 'Day_22', 'Day_23', 'Day_24', 'Day_25', 'Day_26', 'Day_27',\n",
    "       'Day_28', 'Day_29', 'Day_30', 'Day_31', 'Weekday_0', 'Weekday_1',\n",
    "       'Weekday_2', 'Weekday_3', 'Weekday_4', 'Weekday_5', 'Weekday_6',\n",
    "       'is_weekend', 'is_month_start', 'is_month_end', \n",
    "        'New_Year', 'Valentine_Day', 'Days_before_Ramazan', 'Ramazan',\n",
    "        'Eid_al_Fitr', 'Days_after_Eid_al_Fitr', 'Days_before_Eid_al_Adha',\n",
    "        'Eid_al_Adha', 'Muharram', 'Day_before_Independence_day',\n",
    "        'Independence_day', 'Eid_Milad_un_Nabi'\n",
    "    ]\n",
    "\n",
    "    lag_features = [c for c in df.columns if c.startswith('lag_')]\n",
    "    rolling_features = [c for c in df.columns if c.startswith('rolling_')]\n",
    "    features = base_features + lag_features + rolling_features\n",
    "    target = 'Quantity'\n",
    "\n",
    "    df = pd.concat([\n",
    "        df[df[\"InvoiceDate\"] != tomorrow_date].dropna(subset=features + [target]),\n",
    "        df[df[\"InvoiceDate\"] == tomorrow_date]\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    logger.info(\"PreForecastingSteps completed, dataframe ready with %d rows\", len(df))\n",
    "    return df, features, target\n",
    "\n",
    "\n",
    "def calculate_weighted_mda(actual_changes, predicted_changes, prev_actuals):\n",
    "    scores = []\n",
    "    total_weight = sum(prev_actuals) + 1e-6\n",
    "    for actual, predicted, prev in zip(actual_changes, predicted_changes, prev_actuals):\n",
    "        weight = prev / total_weight\n",
    "        sign_match = np.sign(actual) == np.sign(predicted)\n",
    "        error = abs(predicted - actual)\n",
    "        score = (1 / (1 + error)) if sign_match else (0.5 / (1 + error))\n",
    "        scores.append(score * weight)\n",
    "    return np.sum(scores) if scores else np.nan\n",
    "\n",
    "def calculate_rmse(y_true, y_pred):\n",
    "    errors = (y_true - y_pred) ** 2\n",
    "    return np.sqrt(np.mean(errors)) if len(errors) > 0 else np.nan\n",
    "\n",
    "\n",
    "def forecasting(df: pd.DataFrame, features: list, target: str, tomorrow_date):\n",
    "    logger.info(\"Forecasting started for date %s\", tomorrow_date)\n",
    "\n",
    "    param_grid = [\n",
    "        {\"n_estimators\": 150, \"max_depth\": 8, \"max_features\": None,\n",
    "         \"min_samples_split\": 10, \"min_samples_leaf\": 3},\n",
    "        {\"n_estimators\": 200, \"max_depth\": 10, \"max_features\": 0.7,\n",
    "         \"min_samples_split\": 10, \"min_samples_leaf\": 3},\n",
    "        {\"n_estimators\": 300, \"max_depth\": None, \"max_features\": 0.5,\n",
    "         \"min_samples_split\": 12, \"min_samples_leaf\": 4},\n",
    "    ]\n",
    "\n",
    "    val_window_days = 20\n",
    "    calib_window_days = 20\n",
    "    final_result_df = pd.DataFrame()\n",
    "    forecast_date = tomorrow_date\n",
    "    \n",
    "    for i, ((prod, branch), group) in enumerate(df.groupby([\"Product\", \"Branch\"]), start=1):\n",
    "        group = group.sort_values(\"InvoiceDate\").reset_index(drop=True)\n",
    "        logger.info(\"Processing product %d | %s-%s with %d rows\", i, prod, branch, len(group))\n",
    "\n",
    "        # Define windows\n",
    "        calib_start = forecast_date - timedelta(days=calib_window_days)\n",
    "        calib_end = forecast_date - timedelta(days=1)\n",
    "        val_start = calib_start - timedelta(days=val_window_days)\n",
    "        val_end = calib_start - timedelta(days=1)\n",
    "\n",
    "        # Masks\n",
    "        train_mask = (group['InvoiceDate'] < val_start)\n",
    "        val_mask = (group['InvoiceDate'] >= val_start) & (group['InvoiceDate'] <= val_end)\n",
    "        calib_mask = (group['InvoiceDate'] >= calib_start) & (group['InvoiceDate'] <= calib_end)\n",
    "        test_mask = (group['InvoiceDate'] == forecast_date)\n",
    "\n",
    "        X_train, y_train = group.loc[train_mask, features], group.loc[train_mask, target]\n",
    "        X_val, y_val = group.loc[val_mask, features], group.loc[val_mask, target]\n",
    "        X_calib, y_calib = group.loc[calib_mask, features], group.loc[calib_mask, target]\n",
    "        X_test = group.loc[test_mask, features]\n",
    "\n",
    "        if len(X_test) == 0:\n",
    "            logger.warning(\"No test rows for %s-%s on %s — skipping\", prod, branch, forecast_date)\n",
    "            continue\n",
    "\n",
    "        if len(X_train) < 20 or len(X_val) < 10 or len(X_calib) < 10:\n",
    "            logger.warning(\"%s-%s Not enough data in splits (train ≥20, val ≥10, calib ≥10) — skipping\", prod, branch)\n",
    "            continue\n",
    "\n",
    "        # Hyperparameter tuning\n",
    "        results_eval = []\n",
    "        best_params = None\n",
    "        for params in param_grid:\n",
    "            try:\n",
    "                model_tmp = RandomForestRegressor(n_jobs=-1, random_state=42, **params)\n",
    "                model_tmp.fit(X_train.values, y_train.values)\n",
    "                y_val_pred = model_tmp.predict(X_val.values)\n",
    "\n",
    "                # Compute WMDA & RMSE\n",
    "                prev_actual = y_train.iloc[-1]\n",
    "                extended_actuals = np.concatenate([[prev_actual], y_val])\n",
    "                predicted_extended = np.concatenate([[prev_actual], y_val_pred])\n",
    "                actual_changes = (extended_actuals[1:] - extended_actuals[:-1]) / (extended_actuals[:-1] + 1e-6)\n",
    "                predicted_changes = (predicted_extended[1:] - extended_actuals[:-1]) / (extended_actuals[:-1] + 1e-6)\n",
    "                wmda_score = calculate_weighted_mda(actual_changes, predicted_changes, extended_actuals[:-1])\n",
    "                rmse_score = calculate_rmse(y_val.values, y_val_pred)\n",
    "\n",
    "                results_eval.append((wmda_score, rmse_score, params))\n",
    "            except Exception as e:\n",
    "                logger.exception(\"Fit failed for %s-%s with params %s: %s\", prod, branch, params, e)\n",
    "                continue\n",
    "\n",
    "        if not results_eval:\n",
    "            logger.warning(\"All parameter fits failed for %s-%s — skipping\", prod, branch)\n",
    "            continue\n",
    "\n",
    "        # Normalize + hybrid scoring\n",
    "        wmda_vals = np.array([r[0] for r in results_eval])\n",
    "        rmse_vals = np.array([r[1] for r in results_eval])\n",
    "        if np.all(np.isnan(wmda_vals)) or np.all(np.isnan(rmse_vals)):\n",
    "            logger.warning(\"Skipping %s-%s — invalid WMDA/RMSE (all NaN)\", prod, branch)\n",
    "            continue\n",
    "\n",
    "        wmda_low, wmda_high = np.nanpercentile(wmda_vals, 5), np.nanpercentile(wmda_vals, 95)\n",
    "        rmse_low, rmse_high = np.nanpercentile(rmse_vals, 5), np.nanpercentile(rmse_vals, 95)\n",
    "        norm = lambda val, low, high: (val - low) / (high - low + 1e-6)\n",
    "        alpha, beta = 0.8, 1.0\n",
    "        hybrid_scores = [\n",
    "            alpha * norm(w, wmda_low, wmda_high) + beta * (1 - norm(r, rmse_low, rmse_high))\n",
    "            for w, r in zip(wmda_vals, rmse_vals)\n",
    "        ]\n",
    "\n",
    "        if np.all(np.array(hybrid_scores) <= 0):\n",
    "            best_idx = np.nanargmax(wmda_vals)\n",
    "        else:\n",
    "            best_idx = np.nanargmax(hybrid_scores)\n",
    "\n",
    "        best_wmda, best_rmse, best_params = results_eval[best_idx]\n",
    "        best_hybrid = hybrid_scores[best_idx]\n",
    "        # logger.info(\"%s-%s: Selected best params %s | Best WMDA=%.4f | Best RMSE=%.4f | Best hybrid=%.4f\", prod, branch, best_params, best_wmda, best_rmse, best_hybrid)\n",
    "\n",
    "        # Retrain final model on train+val\n",
    "        X_full = pd.concat([X_train, X_val], axis=0)\n",
    "        y_full = pd.concat([y_train, y_val], axis=0)\n",
    "\n",
    "        rf = WrapRegressor(RandomForestRegressor(n_jobs=-1, random_state=42, **best_params, oob_score=True))\n",
    "        rf.fit(X_full.values, y_full.values)\n",
    "        y_pred_train = rf.predict(X_full.values)\n",
    "        residuals = np.abs(y_full - y_pred_train)\n",
    "\n",
    "        de = DifficultyEstimator()\n",
    "        de.fit(X_full.values, residuals=residuals, scaler=True, beta=0.01)\n",
    "        rf.calibrate(X_calib.values, y_calib.values, de=de)\n",
    "\n",
    "        # Predict on X_test\n",
    "        preds = rf.predict(X_test.values)\n",
    "        int_80 = rf.predict_int(X_test.values, confidence=0.8, y_min=0)\n",
    "        int_90 = rf.predict_int(X_test.values, confidence=0.9, y_min=0)\n",
    "        int_70 = rf.predict_int(X_test.values, confidence=0.7, y_min=0)\n",
    "        int_60 = rf.predict_int(X_test.values, confidence=0.6, y_min=0)\n",
    "        int_50 = rf.predict_int(X_test.values, confidence=0.5, y_min=0)\n",
    "\n",
    "        # Attach identifiers and append\n",
    "        temp = group.loc[test_mask, ['InvoiceDate','Product','Branch','TopCategory','Quantity']].copy()\n",
    "        temp['Predicted'] = preds\n",
    "        temp['Lower_90'], temp['Upper_90'] = zip(*int_90)\n",
    "        temp['Lower_80'], temp['Upper_80'] = zip(*int_80)\n",
    "        temp['Lower_70'], temp['Upper_70'] = zip(*int_70)\n",
    "        temp['Lower_60'], temp['Upper_60'] = zip(*int_60)\n",
    "        temp['Lower_50'], temp['Upper_50'] = zip(*int_50)\n",
    "        temp['best_hybrid_score'] = best_hybrid\n",
    "        temp['best_wmda'] = best_wmda\n",
    "        temp['best_rmse'] = best_rmse\n",
    "\n",
    "        if final_result_df.empty:\n",
    "            final_result_df = temp.copy()\n",
    "        else:\n",
    "            final_result_df = pd.concat([final_result_df, temp], ignore_index=True)\n",
    "\n",
    "    logger.info(\"Forecasting completed | Final result rows: %d\", len(final_result_df))\n",
    "    \n",
    "    if final_result_df.empty:\n",
    "        logger.critical(\"Forecasting failed: final_result_df is empty\", exc_info=True)\n",
    "        raise RuntimeError(\n",
    "            \"Forecasting produced no results. Check data availability, \"\n",
    "            \"filters, or model training steps.\"\n",
    "        )\n",
    "    \n",
    "    logger.info(\"Total Unique Products Forecasted: %d\", final_result_df.Product.nunique())\n",
    "    return final_result_df\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_and_validate_dates(df: pd.DataFrame, date_col: str) -> pd.DataFrame:\n",
    "\n",
    "    if date_col == 'InvoiceDate':\n",
    "        df_name = 'sales_df'\n",
    "    elif date_col == 'date':\n",
    "        df_name = 'stocks_df'\n",
    "    else:\n",
    "        df_name = 'unknown_df'\n",
    "        logger.warning(\"Unknown date column '%s' provided\", date_col)\n",
    "        \n",
    "    logger.info(\"Starting date preprocessing for '%s'\", df_name)\n",
    "    \n",
    "    df = df.copy()\n",
    "\n",
    "    try:\n",
    "        # --- Clean up the date strings\n",
    "        df[date_col] = (\n",
    "            df[date_col]\n",
    "            .astype(str)\n",
    "            .str.replace(r'T', ' ', regex=True)\n",
    "            .str.replace(r'(\\.\\d+)$', '', regex=True)\n",
    "        )\n",
    "\n",
    "        # --- Convert to datetime\n",
    "        df[date_col] = pd.to_datetime(df[date_col]).dt.date\n",
    "        df[date_col] = pd.to_datetime(df[date_col])\n",
    "        logger.info(\"Date conversion to datetime completed for '%s' column\", date_col)\n",
    "\n",
    "        # --- Validate that max date equals today's date\n",
    "        max_date = df[date_col].max().date()\n",
    "        logger.info(\n",
    "            \"Max date found in %s: %s | Expected today: %s\",\n",
    "            df_name, max_date, today_date\n",
    "        )\n",
    "\n",
    "        if max_date != today_date and df_name == 'sales_df':\n",
    "            logger.critical(\n",
    "                \"Date validation failed for %s | Max %s = %s, Expected = %s\",\n",
    "                df_name, date_col, max_date, today_date, exc_info=True\n",
    "            )\n",
    "            raise ValueError(\n",
    "                f\"The latest {date_col} ({max_date}) in {df_name} \"\n",
    "                f\"does not match today's date ({today_date}).\"\n",
    "            )\n",
    "\n",
    "        logger.info(\"Date validation Done for %s\", df_name)\n",
    "        return df\n",
    "\n",
    "    except Exception:\n",
    "        logger.critical(\"Preprocessing failed for %s using column '%s'\", df_name, date_col, exc_info=True)\n",
    "        raise\n",
    "#%%\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# SQL Server connection configuration\n",
    "# ============================================================\n",
    "logger.info(\"Initializing database connection\")\n",
    "\n",
    "conn_str = (\n",
    "    \"DRIVER={ODBC Driver 17 for SQL Server};\"\n",
    "    \"SERVER=;\"\n",
    "    \"DATABASE=united_king;\"\n",
    "    \"UID=;\"\n",
    "    \"PWD=;\"\n",
    "    \"Trusted_Connection=no;\"\n",
    "    \"Connection Timeout=600;\"\n",
    ")\n",
    "\n",
    "engine_str = \"mssql+pyodbc:///?odbc_connect=\" + conn_str.replace(\" \", \"%20\")\n",
    "\n",
    "try:\n",
    "    engine = create_engine(engine_str)\n",
    "    with engine.connect() as conn:\n",
    "        logger.info(\"Database connection successful\")\n",
    "except Exception as e:\n",
    "    logger.critical(\"Database connection failed\", exc_info=True)\n",
    "    raise ConnectionError(f\" Database connection failed: {e}\")\n",
    "\n",
    "#%%\n",
    "\n",
    "# ============================================================\n",
    "# Read product and branch filters\n",
    "# ============================================================\n",
    "logger.info(\"Reading product and branch filters\")\n",
    "\n",
    "# Absolute paths to data files\n",
    "products_path = os.path.join(script_dir, \"products.txt\")\n",
    "branches_path = os.path.join(script_dir, \"branches.txt\")\n",
    "\n",
    "try:\n",
    "    with open(products_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        products = list(set(line.strip() for line in f if line.strip()))\n",
    "        logger.info(f\"Loaded {len(products)} products from product.txt\")\n",
    "    \n",
    "    with open(branches_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        branches = list(set(line.strip() for line in f if line.strip()))\n",
    "        logger.info(f\"Loaded {len(branches)} branches from branches.txt\")\n",
    "except Exception as e:\n",
    "    logger.critical(\"Could not load input files\", exc_info=True)\n",
    "    raise RuntimeError(\" File loading failed\") from e   \n",
    "\n",
    "# ============================================================\n",
    "# Load sales data\n",
    "# ============================================================\n",
    "product_params = [f\":p{i}\" for i in range(len(products))]\n",
    "branch_params = [f\":b{i}\" for i in range(len(branches))]\n",
    "\n",
    "query_sales = text(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM sales\n",
    "    WHERE Product IN ({\", \".join(product_params)})\n",
    "      AND Branch IN ({\", \".join(branch_params)});\n",
    "\"\"\")\n",
    "\n",
    "query_stocks = text(\"SELECT * FROM stocks;\")\n",
    "query_master = text(\"SELECT * FROM products_master;\")\n",
    "\n",
    "# Convert literal \"\\xa0\" to real non-breaking space\n",
    "products_clean = [p.encode(\"utf-8\").decode(\"unicode_escape\") for p in products]\n",
    "\n",
    "params = {f\"p{i}\": prod for i, prod in enumerate(products_clean)}\n",
    "params.update({f\"b{i}\": br for i, br in enumerate(branches)})\n",
    "\n",
    "#%%\n",
    "\n",
    "logger.info(\"Starting data extraction from database\")\n",
    "\n",
    "try:\n",
    "    # ============================================================\n",
    "    # Load all necessary tables in a single connection\n",
    "    # ============================================================\n",
    "    with engine.connect() as conn:\n",
    "        sales_df = pd.read_sql(query_sales, conn, params=params)\n",
    "        stocks_df = pd.read_sql(query_stocks, conn)\n",
    "        products_master_df = pd.read_sql(query_master, conn)\n",
    "        festivals_df = pd.read_sql(\"SELECT * FROM festival_calendar;\", conn)\n",
    "\n",
    "    logger.info(\n",
    "        f\"Data loaded | sales: {len(sales_df)}, stocks: {len(stocks_df)}, master: {len(products_master_df)}, festivals: {len(festivals_df)}\"\n",
    "    )\n",
    "\n",
    "    # ============================================================\n",
    "    # Preprocess dates\n",
    "    # ============================================================\n",
    "    sales_df = preprocess_and_validate_dates(sales_df, 'InvoiceDate')\n",
    "    stocks_df = preprocess_and_validate_dates(stocks_df, 'date')\n",
    "    festivals_df['date'] = pd.to_datetime(festivals_df['date'])\n",
    "\n",
    "    if sales_df.empty:\n",
    "        logger.critical(\"Sales dataframe is empty after preprocessing — stopping pipeline\", exc_info=True)\n",
    "        raise RuntimeError(\" No sales data available for processing.\")\n",
    "        \n",
    "    missed_prod = set(products_clean) - set(sales_df.Product.unique())\n",
    "    if missed_prod:\n",
    "        logger.warning(\"Products missing in sales data: %s\", missed_prod)\n",
    "\n",
    "    # ============================================================\n",
    "    # Preprocessing steps\n",
    "    # ============================================================\n",
    "    processed_df, tomorrow_date = preProcessingSteps(sales_df)\n",
    "\n",
    "    if processed_df.empty:\n",
    "        logger.critical(\"Processed dataframe is empty — stopping pipeline\", exc_info=True)\n",
    "        raise RuntimeError(\" No processed data available to merge or forecast.\")\n",
    "\n",
    "    # ============================================================\n",
    "    # Merge with festivals\n",
    "    # ============================================================\n",
    "    logger.info(\"Merging processed data with festival calendar\")\n",
    "    processed_df = processed_df.merge(\n",
    "        festivals_df,\n",
    "        how='left',\n",
    "        left_on='InvoiceDate',\n",
    "        right_on='date'\n",
    "    ).drop(columns=['date'])\n",
    "    logger.info(\"Festival merge completed\")\n",
    "\n",
    "    # ============================================================\n",
    "    # Pre-forecasting steps\n",
    "    # ============================================================\n",
    "    df, features, target = preForecastingSteps(processed_df, tomorrow_date)\n",
    "\n",
    "except Exception:\n",
    "    logger.critical(\"Pipeline failed unexpectedly\", exc_info=True)\n",
    "    raise\n",
    "\n",
    "#%%\n",
    "\n",
    "prediction_results = forecasting(df, features, target, tomorrow_date)\n",
    "stocks_df = stocks_df.rename(columns={'date': 'InvoiceDate'})\n",
    "\n",
    "#%%\n",
    "\n",
    "socks_and_masters = stocks_df.merge(products_master_df[['Id', 'Name', 'IsDecimalAllow']].rename(columns={'Id': 'id', 'Name': 'Product'}),\n",
    "                                    on='id', how='outer')\n",
    "\n",
    "socks_and_masters = socks_and_masters[(socks_and_masters['CompanyBranch'].isin(branches)) & (socks_and_masters['Product'].isin(products_clean))].rename(columns={'CompanyBranch': 'Branch'})\n",
    "\n",
    "prediction_results = prediction_results[['InvoiceDate', 'Product', 'Branch', 'TopCategory', 'Predicted', 'Upper_80']].copy()\n",
    "prediction_results['Run_Date'] = pd.to_datetime(today_date)\n",
    "\n",
    "order_qty = (socks_and_masters.rename(columns={'InvoiceDate': 'Run_Date'})\n",
    "             .merge(\n",
    "                 prediction_results, \n",
    "                 on=['Product', 'Branch', 'Run_Date'], \n",
    "                 how='right'\n",
    "                )\n",
    ")\n",
    "\n",
    "order_qty['Inventory'] = order_qty['Inventory'].clip(lower=0).fillna(0)\n",
    "\n",
    "shelf_life = 1\n",
    "# Round conditionally based on 'IsDecimalAllow'\n",
    "order_qty['IsDecimalAllow'] = order_qty['IsDecimalAllow'].fillna(0)\n",
    "order_qty['Predicted'] = np.where(\n",
    "    order_qty['IsDecimalAllow'] == 1,\n",
    "    order_qty['Predicted'].round(2),\n",
    "    order_qty['Predicted'].round()\n",
    ")\n",
    "\n",
    "order_qty['Upper_80'] = np.where(\n",
    "    order_qty['IsDecimalAllow'] == 1,\n",
    "    order_qty['Upper_80'].round(2),\n",
    "    order_qty['Upper_80'].round()\n",
    ")\n",
    "\n",
    "order_qty['order_qty_upper80'] = np.where(\n",
    "    order_qty['IsDecimalAllow'] == 1,\n",
    "    (order_qty['Upper_80'] - order_qty['Inventory']).clip(lower=0).round(2),\n",
    "    (order_qty['Upper_80'] - order_qty['Inventory']).clip(lower=0).round()\n",
    ")\n",
    "\n",
    "order_qty['order_qty_predicted'] = np.where(\n",
    "    order_qty['IsDecimalAllow'] == 1,\n",
    "    ((order_qty['Predicted'] * shelf_life) - order_qty['Inventory']).clip(lower=0).round(2),\n",
    "    ((order_qty['Predicted'] * shelf_life) - order_qty['Inventory']).clip(lower=0).round()\n",
    ")\n",
    "\n",
    "order_qty = order_qty[['id', 'Product', 'Branch', 'CompanyBranchId', 'InvoiceDate', 'TopCategory', 'Inventory', 'Predicted', 'Upper_80', 'order_qty_upper80', 'order_qty_predicted', 'Run_Date']]\n",
    "order_qty.columns = order_qty.columns.str.lower()\n",
    "\n",
    "logger.info(\"Order Quantity Created\")\n",
    "\n",
    "#%%\n",
    "\n",
    "# # Pushing Results To SQL\n",
    "conn = pyodbc.connect(conn_str)\n",
    "conn.autocommit = True  # ensures fast insert\n",
    "\n",
    "def write_sql_pyodbc(df, table, conn):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.fast_executemany = True\n",
    "    \n",
    "    # Get columns from table\n",
    "    cursor.execute(f\"SELECT TOP 0 * FROM {table}\")\n",
    "    sql_cols = [desc[0] for desc in cursor.description]\n",
    "    \n",
    "    # Ensure column order matches\n",
    "    df = df[sql_cols]\n",
    "    \n",
    "    placeholders = \", \".join([\"?\"] * len(sql_cols))\n",
    "    columns_str = \", \".join(f\"[{col}]\" for col in sql_cols)\n",
    "    sql = f\"INSERT INTO {table} ({columns_str}) VALUES ({placeholders})\"\n",
    "    \n",
    "    cursor.executemany(sql, df.values.tolist())\n",
    "    cursor.close()\n",
    "\n",
    "def exec_sql(query):\n",
    "    cursor = conn.cursor()\n",
    "    # Execute with parameter to prevent SQL injection\n",
    "    cursor.execute(query)\n",
    "    # Commit the changes\n",
    "    conn.commit()\n",
    "    # Close the cursor and connection\n",
    "    cursor.close()\n",
    "\n",
    "for col in list(order_qty.columns):\n",
    "    order_qty[col] = order_qty[col].astype(str).replace({\"\": None, \"nan\": None, \"NaT\": None, \"None\": None})\n",
    "\n",
    "logger.info(f\"Checking existing records in results table for run_date={today_date}\")\n",
    "\n",
    "check_query = f\"\"\"\n",
    "SELECT COUNT(1)\n",
    "FROM results\n",
    "WHERE run_date = '{today_date}'\n",
    "\"\"\"\n",
    "\n",
    "delete_query = f\"\"\"\n",
    "DELETE FROM results \n",
    "WHERE run_date = '{today_date}'\n",
    "\"\"\"\n",
    "\n",
    "existing_count = pd.read_sql(check_query, engine).iloc[0, 0]\n",
    "\n",
    "if existing_count > 0:\n",
    "    logger.warning(\n",
    "        f\"Existing records found for run_date={today_date} | rows={existing_count}. Deleting before insert.\"\n",
    "    )\n",
    "    exec_sql(delete_query)\n",
    "    logger.info(\"Old records deleted successfully\")\n",
    "else:\n",
    "    logger.info(\"No existing records found for today — fresh insert\")\n",
    "\n",
    "rows_to_insert = len(order_qty)\n",
    "logger.info(f\"Uploading predictions to results table | rows={rows_to_insert}\")\n",
    "\n",
    "write_sql_pyodbc(order_qty, \"results\", conn)\n",
    "\n",
    "logger.info(\"Predictions successfully uploaded to results table\")\n",
    "\n",
    "read_query = f\"\"\"SELECT [id]\n",
    "                      ,[product]\n",
    "                      ,[branch]\n",
    "                      ,[invoicedate]\n",
    "                      ,[topcategory]\n",
    "                      ,[inventory]\n",
    "                      ,[predicted]\n",
    "                      ,[order_qty_predicted]\n",
    "                      ,[upper_80]\n",
    "                      ,[order_qty_upper80]\n",
    "                  FROM [united_king].[dbo].[results]\n",
    "                  WHERE [run_date] = '{today_date}'\"\"\"\n",
    "                  \n",
    "result_df = pd.read_sql(read_query, engine)\n",
    "result_df['IsInvReceived'] = np.where(result_df['inventory'] > 0, 'Yes', 'No')\n",
    "\n",
    "result_df.to_excel(\"united_king_predictions.xlsx\", index=False)\n",
    "\n",
    "logger.info(\"Sent results to Excel file: united_king_predictions.xlsx, Saved at %s\", os.getcwd())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
